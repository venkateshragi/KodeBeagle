/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.kodebeagle.ml

import scala.Iterator
import scala.annotation.elidable
import scala.annotation.elidable.ASSERTION
import scala.collection.mutable
import org.apache.commons.lang.NotImplementedException
import org.apache.spark.Logging
import org.apache.spark.SparkContext
import org.apache.spark.graphx.Edge
import org.apache.spark.graphx.EdgeTriplet
import org.apache.spark.graphx.Graph
import org.apache.spark.graphx.PartitionID
import org.apache.spark.graphx.PartitionStrategy
import org.apache.spark.graphx.TripletFields
import org.apache.spark.graphx.VertexId
import org.apache.spark.graphx.VertexRDD
import org.apache.spark.rdd.RDD
import org.apache.spark.rdd.RDD.rddToPairRDDFunctions
import scala.util.Random
import scala.collection.mutable.PriorityQueue
import org.apache.spark.rdd.EmptyRDD

/**
 *
 * Latent Dirichlet Allocation (LDA), a topic model designed for text documents.
 *
 * Terminology:
 *  - "word" = "term": an element of the vocabulary
 *  - "token": instance of a term appearing in a document
 *  - "topic": multinomial distribution over words representing some concept
 *
 * Currently, the underlying implementation uses Expectation-Maximization (EM), implemented
 * according to the Asuncion et al. (2009) paper referenced below.
 *
 * References:
 *  - Original LDA paper (journal version):
 *    Blei, Ng, and Jordan.  "Latent Dirichlet Allocation."  JMLR, 2003.
 *     - This class implements their "smoothed" LDA model.
 *  - Paper which clearly explains several algorithms, including EM:
 *    Asuncion, Welling, Smyth, and Teh.
 *    "On Smoothing and Inference for Topic Models."  UAI, 2009.
 *
 * Note: This implementation uses the basic implementation of LDA as here by github user
 * EntilZha and builds on top of it.
 * See LDA.scala in https://github.com/EntilZha/spark/blob/LDA-Refactor/mllib/
 */

class LDA private (
  private var k: Int,
  private var maxIterations: Int,
  private var checkPointInterval: Int,
  private var topicSmoothing: Array[Double],
  private var termSmoothing: Array[Double],
  private var seed: Long,
  private var algorithm: LDA.LearningAlgorithm.LearningAlgorithm) extends Logging {

  import LDA._

  def this() = this(k = 3, maxIterations = 20, checkPointInterval = 100,
    topicSmoothing = Array(), termSmoothing = Array(), seed = Random.nextLong(),
    algorithm = LDA.LearningAlgorithm.Gibbs)

  /**
   * Number of background topics to infer.  I.e., the number of soft cluster centers.
   * (default = 3)
   */
  def getK: Int = k

  def setK(k: Int): this.type = {
    require(k > 0, s"LDA k (number of clusters) must be > 0, but was set to $k")
    this.k = k
    this
  }

  /**
   * Topic smoothing parameter (commonly named "alpha").
   *
   * This is the parameter to the Dirichlet prior placed on the per-document topic distributions
   * ("theta").  We use a symmetric Dirichlet prior.
   *
   * This value should be > 0.0, where larger values mean more smoothing (more regularization).
   * If set to -1, then topicSmoothing is set automatically.
   *  (default = -1 = automatic)
   *
   * Automatic setting of parameter:
   *  - For EM: default = (50 / k) + 1.
   *     - The 50/k is common in LDA libraries.
   *     - The +1 follows Asuncion et al. (2009), who recommend a +1 adjustment for EM.
   */
  def getTopicSmoothing: Array[Double] = topicSmoothing

  def getTopicSmoothing(nt: Int): Array[Double] = {
    val topicSmoothing = if (this.topicSmoothing.size == nt) {
      this.topicSmoothing
    } else {
      // (50.0 / k) + 1.0
      val alpham = new Array[Double](nt)
      var i = 0
      while (i < k) {
        // alpham(i) = (50.0 / k) + 1.0
        alpham(i) = 1.7
        i = i + 1
      }
      while (i < nt) {
        // alpham(i) = (50.0 / nVisibleTopics) + 1.0
        alpham(i) = 2.3
        i = i + 1
      }
      alpham
    }
    topicSmoothing
  }

  def setTopicSmoothing(topicSmoothing: Array[Double]): this.type = {
    require(topicSmoothing.size == 0 || topicSmoothing.filter(x => x > 0.0).size > 0,
      s"LDA topicSmoothing weights must be > 0 (or empty for auto), but was set to $topicSmoothing")
    // TODO: Check each value too
    this.topicSmoothing = topicSmoothing
    this
  }

  /**
   * Term smoothing parameter (commonly named "eta").
   *
   * This is the parameter to the Dirichlet prior placed on the per-topic word distributions
   * (which are called "beta" in the original LDA paper by Blei et al., but are called "phi" in many
   *  later papers such as Asuncion et al., 2009.)
   *
   * This value should be > 0.0.
   * If set to -1, then termSmoothing is set automatically.
   *  (default = -1 = automatic)
   *
   * Automatic setting of parameter:
   *  - For EM: default = 0.1 + 1.
   *     - The 0.1 gives a small amount of smoothing.
   *     - The +1 follows Asuncion et al. (2009), who recommend a +1 adjustment for EM.
   */
  def getTermSmoothing: Array[Double] = termSmoothing

  def getTermSmoothing(nt: Int): Array[Double] = {
    val termSmoothing = if (this.termSmoothing.size == nt) {
      this.termSmoothing
    } else {
      // 1.1
      val betam = new Array[Double](nt)
      var i = 0
      while (i < k) {
        betam(i) = 1.1
        i = i + 1
      }
      while (i < nt) {
        betam(i) = 0.1
        i = i + 1
      }
      betam
    }
    termSmoothing
  }

  def setTermSmoothing(termSmoothing: Array[Double]): this.type = {
    require(termSmoothing.size == 0 || termSmoothing.filter(x => x > 0.0).size > 0,
      s"LDA termSmoothing must be > 0 (or empty for auto), but was set to $termSmoothing")
    // TODO: check each element
    this.termSmoothing = termSmoothing
    this
  }

  /**
   * Maximum number of iterations for learning.
   * (default = 20)
   */
  def getMaxIterations: Int = maxIterations

  def setMaxIterations(maxIterations: Int): this.type = {
    this.maxIterations = maxIterations
    this
  }

  /**
   * Checkpoint interval.
   * (default = every 100 iterations)
   */
  def getCheckPointInterval: Int = checkPointInterval

  def setCheckPointInterval(checkPointInterval: Int): this.type = {
    require(checkPointInterval > 0, "Checkpoint interval should be a positive value")
    this.checkPointInterval = checkPointInterval
    this
  }

  /** Random seed */
  def getSeed: Long = seed

  def setSeed(seed: Long): this.type = {
    this.seed = seed
    this
  }

  def runFromEdges(edges: RDD[(WordId, DocId)],
    documentGroupings: Option[RDD[(DocId, GroupId)]] = None): DistributedLDAModel = {
    // No. of document specific topics: should be same for all documents 
    var distinctDocPvtTopics = 0
    var perDocPvtTopics = 0
    var groupedDocsOpt: Option[RDD[(DocId, Array[GroupId])]] = None
    // Find out the groups to which each document is assigned. 
    // Their number should be same for all documents
    if (documentGroupings.isDefined) {
      val groupedDocs = documentGroupings.get.map(f => (-(f._1 + 1), f._2)).groupBy(_._1).map({
        case (docId, itr) =>
          (docId, itr.map(_._2).toArray)
      })
      val max = groupedDocs.map(_._2.length).max
      val min = groupedDocs.map(_._2.length).min

      assert(max == min,
        s"Each document is not assigned to an equal number of groups. Max: $max, and Min: $min")
      distinctDocPvtTopics = groupedDocs.map(_._2).
        flatMap { x => x.toList }.distinct().count().toInt
      perDocPvtTopics = max
      groupedDocsOpt = Option(groupedDocs)
    }

    // Topics visible to words in a doc : public topics + doc specific topics.
    val nVisibleTopics = k + perDocPvtTopics
    // Initialize parameters
    val topicSmoothing: Array[Double] = getTopicSmoothing(nVisibleTopics)
    val termSmoothing: Array[Double] = getTermSmoothing(nVisibleTopics)

    assert(topicSmoothing.length == termSmoothing.length
      && termSmoothing.length == (k + perDocPvtTopics),
      "Improper setup of parameters")

    var state = algorithm match {
      case LearningAlgorithm.Gibbs => LDA.GibbsLearningState.initialStateFromEdges(
        edges, groupedDocsOpt, k,
        distinctDocPvtTopics, perDocPvtTopics, topicSmoothing,
        termSmoothing, seed)
    }

    var iter = 0
    while (iter < maxIterations) {
      if (iter % checkPointInterval == 0 && iter > 0) checkpoint(state)
      state = state.next()
      iter += 1
    }
    new DistributedLDAModel(state)
  }

  def checkpoint(state: LDA.GibbsLearningState): Unit = {

    /**
     * Need to do this because - if an RDD is materialized before
     * marking it for checkpoint the checkpointing doesnt work.
     */
    state.graph = Graph(state.graph.vertices, state.graph.edges)
      .partitionBy(PartitionStrategy.EdgePartition1D)
    val g = state.graph
    val e = g.edges
    val v = g.vertices
    g.checkpoint()
    v.count()
    e.count()
    assert(state.graph.isCheckpointed, "Graph not checkpointed")
  }
}

/**
 * LDA contains utility methods used in the LDA class. These are mostly
 * methods which will be serialized during computation so cannot be methods.
 */
object LDA {
  object LearningAlgorithm extends Enumeration {
    type LearningAlgorithm = Value
    val Gibbs, EM = Value
  }
  type TopicCounts = Array[Int]
  type DocId = Long
  type WordId = Long
  type GroupId = Long
  def isDocumentVertex(v: Long): Boolean = v < 0
  def isWordVertex(v: Long): Boolean = v >= 0
  trait LearningState {
    def logPrior(): Double
    def logLikelihood(): Double
    def collectTopicTotals(): TopicCounts
    def describeTopics(maxTermsPerTopic: Long): Array[Array[(Int, Long)]]
    def summarizeDocGroups(groupIndex: Int): Array[(GroupId, DocId, Double)]
    def next(): LearningState
    def vocabSize(): Long
    def k: Int
  }

  trait LearningStateInitialization {
    def initialStateFromEdges(edges: RDD[(WordId, DocId)],
      groupedDocs: Option[RDD[(Long, Array[GroupId])]],
      k: Int,
      distinctDocPvtTopics: Int,
      perDocPvtTopics: Int,
      alpha: Array[Double],
      beta: Array[Double],
      seed: Long): LearningState
  }

  object GibbsLearningState extends Serializable with LearningStateInitialization {
    /**
     * These two variables are used for turning indexing on and off when creating new histograms
     */
    val IndexTrue: DocId = -1L
    val IndexFalse: WordId = 1L
    /**
     * Topic Long is composed of two Ints representint topics.
     * The left Int represents the current topic, right Int represents prior topic.
     */
    type Topic = Long

    case class Histogram(counts: TopicCounts, docTopics: Array[Long]) extends Serializable

    case class Posterior(docs: VertexRDD[Histogram], words: VertexRDD[Histogram])

    def getCurrentTopic(topic: Topic): Int = {
      (topic >> 32).asInstanceOf[Int]
    }

    def getOldTopic(topic: Topic): Int = {
      topic.asInstanceOf[Int]
    }

    def combineTopics(currentTopic: Int, oldTopic: Int): Topic = {
      (currentTopic.asInstanceOf[Long] << 32) | (oldTopic & 0xffffffffL)
    }

    /**
     * Sums two factors together into a, then returns it. This increases memory efficiency
     * and reduces garbage collection.
     * @param a First factor
     * @param b Second factor
     * @return Sum of factors
     */
    def combineCounts(a: TopicCounts, b: TopicCounts): TopicCounts = {
      assert(a.size == b.size)
      var i = 0
      while (i < a.size) {
        a(i) += b(i)
        i += 1
      }
      a
    }

    /**
     * Combines a topic into a factor
     * @param a Factor to add to
     * @param topic topic to add
     * @return Result of adding topic into factor.
     */
    def combineTopicIntoCounts(a: TopicCounts, topic: Topic): TopicCounts = {
      a(getCurrentTopic(topic)) += 1
      a
    }

    def combineDeltaIntoCounts(a: TopicCounts, topic: Topic): TopicCounts = {
      a(getOldTopic(topic)) -= 1
      a(getCurrentTopic(topic)) += 1
      a
    }

    /**
     * Creates a new histogram then applies the deltas in place to maintain the
     * argsort order. It is assumed that deltas are sparse so that
     * the bubblesort used fast.
     * @param oldHistogram
     * @param deltas
     * @return
     */
    def applyDeltasToHistogram(oldHistogram: Histogram,
      deltas: Array[Int],
      vid: Long): Histogram = {
      val counts = new TopicCounts(oldHistogram.counts.length)
      var i = 0
      while (i < counts.length) {
        counts(i) = oldHistogram.counts(i) + deltas(i)
        assert(counts(i) >= 0,
          "\n Vid : " + vid +
            "\n Old histogram: " + oldHistogram.counts.mkString(",") +
            "\n Delta :" + deltas.mkString(" , "))
        i += 1
      }
      makeHistogramFromCounts(counts, oldHistogram.docTopics)
    }

    /**
     * Creates a factor with topic added to it.
     * @param nTopics Number of topics
     * @param topic Topic to start with
     * @return New factor with topic added to it
     */
    def makeCountsFromTopic(nTopics: Int, topic: Topic): TopicCounts = {
      val counts = new TopicCounts(nTopics)
      counts(getCurrentTopic(topic)) += 1
      counts
    }

    /**
     * Creates a new histogram from counts, counts becomes part of the new histogram
     * object.
     * @param counts
     * @param vid
     * @param parameter Parameter for computing norm sum, either alpha or beta
     * @return
     */
    def makeHistogramFromCounts(counts: TopicCounts, docSpecificTopics: Array[Long]): Histogram = {
      Histogram(counts, docSpecificTopics)
    }

    def makeEmptyHistogram(nTopics: Int, nDocSpecificTopics: Int): Histogram = {
      Histogram(new TopicCounts(nTopics), new Array[Long](nDocSpecificTopics))
    }

    /**
     * Extracts the vocabulary from the RDD of tokens. Returns a Map from each word to its unique
     * number key, and an array indexable by that number key to the word
     * @param tokens RDD of tokens to create vocabulary from
     * @return array and map for looking up words from keys and keys from words.
     */
    def extractVocab(tokens: RDD[String]): (Array[String], mutable.Map[String, WordId]) = {
      val vocab = tokens.distinct().collect()
      var vocabLookup = mutable.Map[String, WordId]()
      for (i <- 0 to vocab.length - 1) {
        vocabLookup += (vocab(i) -> i)
      }
      (vocab, vocabLookup)
    }

    /**
     * Extracts edges from an RDD of documents. Each document is a single line/string
     * in the RDD
     * @param lines RDD of documents
     * @param vocab Vocabulary in the documents
     * @param vocabLookup Vocabulary lookup in the documents
     * @param delimiter Delimiter to split on
     * @return RDD of edges between words and documents representing tokens.
     */
    def edgesFromTextDocLines(lines: RDD[String],
      vocab: Array[String],
      vocabLookup: mutable.Map[String, WordId],
      delimiter: String = " "): RDD[(WordId, DocId)] = {
      val sc = lines.sparkContext
      val numDocs = lines.count()
      val docsWithIds = lines.zipWithUniqueId()
      val edges: RDD[(WordId, DocId)] = docsWithIds.flatMap {
        case (line: String, docId: DocId) =>
          val words = line.split(delimiter)
          val docEdges = words.map(word => (vocabLookup(word), docId))
          docEdges
      }
      edges
    }

    /**
     * Re-samples the given token/triplet to a new topic
     * @param randomDouble Random number generator
     * @param totalHistogram roadcast Total histogram of topics
     * @param nt Number of topics
     * @param alpha Parameter for dirichlet prior on per document topic distributions
     * @param beta Parameter for the dirichlet prior on per topic word distributions
     * @param nw Number of words in corpus
     * @return New topic for token/triplet
     */
    def sampleToken(randomDouble: Double,
      topic: Topic,
      docHistogram: Histogram,
      wordHistogram: Histogram,
      totalHistogram: Histogram,
      nt: Int,
      pvtTopics: Array[Long],
      alpha: Array[Double],
      beta: Array[Double],
      nw: Long): Topic = {
      val totalCounts = totalHistogram.counts
      val wHist = wordHistogram.counts
      val dHist = docHistogram.counts
      val oldTopic = getCurrentTopic(topic)
      val nVisibleTopics = nt + pvtTopics.length
      assert(wHist(oldTopic) > 0,
        "Illegal state: word assigned to topic for which histogram count is 0.")
      assert(dHist(oldTopic) > 0)
      assert(totalCounts(oldTopic) > 0)
      assert(nVisibleTopics == alpha.length && nVisibleTopics == beta.length)
      // Construct the conditional
      val conditional = new Array[Double](nVisibleTopics)
      var t = 0
      var conditionalSum = 0.0
      while (t < conditional.size) {
        val index = if (t < nt) t else pvtTopics(t - nt).toInt
        val cavityOffset = if (index == oldTopic) 1 else 0
        val w = wHist(index) - cavityOffset
        val d = dHist(index) - cavityOffset
        val total = totalCounts(index) - cavityOffset
        conditional(t) = (alpha(t) + d) * (beta(t) + w) / (beta(t) * nw + total)
        conditionalSum += conditional(t)
        t += 1
      }
      assert(conditionalSum > 0.0)
      // Generate a random number between [0, conditionalSum)
      val u = randomDouble * conditionalSum
      assert(u < conditionalSum)
      // Draw the new topic from the multinomial
      var newTopic = 0
      var cumsum = conditional(newTopic)
      while (cumsum < u) {
        newTopic += 1
        cumsum += conditional(newTopic)
      }
      newTopic = if (newTopic < nt) newTopic else pvtTopics(newTopic - nt).toInt
      combineTopics(newTopic, oldTopic)
    }

    def countWithoutTopic(counts: TopicCounts, index: Int, topic: Int): Int = {
      if (index == topic) {
        counts(index) - 1
      } else {
        counts(index)
      }
    }

    def vertices(edges: RDD[Edge[Topic]],
      getVertex: Edge[Topic] => VertexId,
      nTopics: Int,
      nDocTopics: Int,
      vertexType: VertexId): RDD[(Long, Histogram)] = {
      edges.map { e => (getVertex(e), e) }.aggregateByKey(makeEmptyHistogram(nTopics, nDocTopics))({
        case (histogram, edge) =>
          val t = getCurrentTopic(edge.attr)
          histogram.counts(t) += 1
          histogram
      }, {
        case (l, r) =>
          Histogram(combineCounts(l.counts, r.counts), l.docTopics)
      })
    }

    def initialStateFromEdges(edges: RDD[(WordId, DocId)],
      groupedDocs: Option[RDD[(Long, Array[GroupId])]],
      k: Int,
      distinctDocPvtTopics: Int,
      perDocPvtTopics: Int,
      alpha: Array[Double],
      beta: Array[Double],
      seed: Long): GibbsLearningState = {
      val state = new GibbsLearningState(edges, groupedDocs, k,
        distinctDocPvtTopics, perDocPvtTopics, alpha, beta)
      state.setup()
      state
    }
  }

  /**
   * LDA contains the model for topic modeling using Latent Dirichlet Allocation
   * @param tokens RDD of edges, transient to insure it doesn't get sent to workers
   * @param nTopics Number of topics
   * @param alpha Model parameter, governs sparsity in document-topic mixture
   * @param beta Model parameter, governs sparsity in word-topic mixture
   * @param loggingInterval Interval for logging
   * @param loggingLikelihood If true, log the likelihood
   * @param loggingTime if true, log the runtime of each component
   */
  private[ml] class GibbsLearningState(@transient val tokens: RDD[(WordId, DocId)],
    @transient val docGroups: Option[RDD[(Long, Array[GroupId])]],
    val nTopics: Int = 100,
    val nDocTopics: Int = 0,
    val nPrivateTopics: Int = 0,
    val alpha: Array[Double],
    val beta: Array[Double],
    val loggingInterval: Int = 0,
    val loggingLikelihood: Boolean = false,
    val loggingTime: Boolean = false) extends LearningState with Serializable with Logging {
    import GibbsLearningState._  
    // var timer: TimeTracker = null
    private var sc: Option[SparkContext] = None
    // Create an empty graph to keep scalastyle happy for null.
    var graph: Graph[Histogram, Topic] = Graph(tokens.sparkContext.emptyRDD[(Long, Histogram)], 
        tokens.sparkContext.emptyRDD[Edge[Topic]])
    var nWords: Long = 0
    var nDocs: Long = 0
    var nTokens: Long = 0
    var totalHistogram: Option[Histogram] = None
    var resampleTimes: Option[mutable.ListBuffer[Long]] = None
    var updateCountsTimes: Option[mutable.ListBuffer[Long]] = None
    var globalCountsTimes: Option[mutable.ListBuffer[Long]] = None
    var likelihoods: Option[mutable.ListBuffer[Double]] = None
    private var iteration = 0
    var modelIsSetup: Boolean = false

    def describeTopics(maxTermsPerTopic: Long): Array[Array[(Int, Long)]] = {
      topWords(maxTermsPerTopic.toInt)
    }

    def collectTopicTotals(): TopicCounts = throw new NotImplementedError()

    def logPrior(): Double = throw new NotImplementedError()

    def k: Int = nTopics

    def vocabSize(): Long = {
      nWords
    }

    /**
     * Get the word vertices by filtering on non-negative vertices
     * @return Word vertices
     */
    def wordVertices: VertexRDD[Histogram] = graph.vertices.filter { case (vid, c) => vid >= 0 }

    /**
     * Get the document vertices by filtering on negative vertices
     * @return Document vertices
     */
    def docVertices: VertexRDD[Histogram] = graph.vertices.filter { case (vid, c) => vid < 0 }

    def setup(): Unit = {
      // timer = new TimeTracker()
      resampleTimes = Option(new mutable.ListBuffer[Long]())
      updateCountsTimes = Option(new mutable.ListBuffer[Long]())
      globalCountsTimes = Option(new mutable.ListBuffer[Long]())
      likelihoods = Option(new mutable.ListBuffer[Double]())
      // timer.start("setup")
      logInfo("Starting LDA setup")
      sc = Option(tokens.sparkContext)
      /**
       * The bipartite terms by document graph.
       */
      graph = createInitialGraph().cache()

      nWords = wordVertices.count()
      nDocs = docVertices.count()
      nTokens = graph.edges.count()
      /**
       * The total counts for each topic
       */
      val counts = graph.edges.map(e => e.attr)
        .aggregate(new TopicCounts(nTopics + nDocTopics))(combineTopicIntoCounts, combineCounts)
      totalHistogram = Option(makeHistogramFromCounts(
        graph.edges.map(e => e.attr)
          .aggregate(new TopicCounts(nTopics + nDocTopics))(combineTopicIntoCounts, combineCounts),
        Array()))
      logInfo("LDA setup finished")
      // timer.stop("setup")
      modelIsSetup = true

    }

    def createInitialGraph(): Graph[Histogram, Topic] = {
      val nT = nTopics
      val nd = nDocTopics
      val nPt = nPrivateTopics
      // To setup a bipartite graph it is necessary to ensure that the document and
      // word ids are in a different namespace
      val edges: RDD[Edge[Topic]] = tokens.mapPartitionsWithIndex({
        case (pid, iterator) =>
          val gen = new java.util.Random(pid)
          iterator.map({
            case (wordId, docId) =>
              assert(wordId >= 0)
              assert(docId >= 0)
              val newDocId: DocId = -(docId + 1L)
              Edge(wordId, newDocId, combineTopics(gen.nextInt(nT + nPt), 0))
          })
      })
      edges.cache
      val setupWordVertices = vertices(edges, _.srcId, nT + nd, nPt, IndexFalse)
      var setupDocVertices = vertices(edges, _.dstId, nT + nd, nPt, IndexTrue)

      // At each doc vertex store what are the doc specific topics.
      if (docGroups.isDefined) {
        val docVerticesWithGroups = setupDocVertices.leftOuterJoin(docGroups.get).map({
          case (docId, (histogram, docArr)) =>
            (docId, Histogram(histogram.counts, docArr.get))
        })

        setupDocVertices = docVerticesWithGroups
      }

      var g = Graph(setupDocVertices ++ setupWordVertices, edges)
        .partitionBy(PartitionStrategy.EdgePartition1D)

      // Assign the doc specific topics correctly here.  
      g = randomTopicCorrection(g, nT)

      // end

      /* g.triplets.foreach(f=>{
          val curr = getCurrentTopic(f.attr)
          assert(curr < nT || 
              (f.dstAttr.docTopics.toSeq.contains(curr) && f.srcAttr.counts(curr) > 0) , 
              s"Edges not properly setup, indicates a bug." + 
              s"$curr , $nT: nT , available doc topics: ${f.dstAttr.docTopics.mkString(",")}" )
        }) */
      g
    }

    def randomTopicCorrection(graph: Graph[Histogram, Topic], nT: Int): Graph[Histogram, Topic] = {
      var g = graph.mapTriplets(et => {
        val curr = getCurrentTopic(et.attr)
        val docTopicIndex = curr - nT
        if (docTopicIndex >= 0) {
          val properTopic = et.dstAttr.docTopics(docTopicIndex).toInt
          // Assign proper doc topic id
          et.attr = combineTopics(properTopic, curr)
        }
        et.attr
      }).cache

      // start : update histograms where words were reassigned to doc specific topics.
      // TODO: Use the same methods as in next()
      val deltas = g.edges.flatMap(e => {
        val topic = e.attr
        val old = getOldTopic(topic)
        val current = getCurrentTopic(topic)
        var result: Iterator[(VertexId, Topic)] = Iterator.empty
        if (old != current && old != 0) {
          result = Iterator((e.srcId, e.attr), (e.dstId, e.attr))
        }
        result
      }).aggregateByKey(new Array[Int](nT + nDocTopics))(combineDeltaIntoCounts, combineCounts)

      g = g.outerJoinVertices(deltas)({ (vid, oldHistogram, vertexDeltasOption) =>
        if (vertexDeltasOption.isDefined) {
          val vertexDeltas = vertexDeltasOption.get
          val histogram = applyDeltasToHistogram(oldHistogram, vertexDeltas, vid)
          histogram
        } else {
          Histogram(oldHistogram.counts, oldHistogram.docTopics)
        }
      }).cache()

      g
    }

    def next(): GibbsLearningState = {

      // Log the negative log likelihood
      if (loggingLikelihood && iteration % loggingInterval == 0) {
        val likelihood = logLikelihood()
        likelihoods.get += likelihood
      }
      // Shadowing because scala's closure capture would otherwise serialize the model object
      val a = alpha
      val b = beta
      val nt = nTopics
      val nd = nDocTopics
      val nw = nWords

      // Broadcast the topic histogram
      val totalHistogramBroadcast = sc.get.broadcast(totalHistogram)

      // Re-sample all the tokens
      var tempTimer: Long = System.nanoTime()
      val parts = graph.edges.partitions.size
      val interIter = iteration
      graph = graph.mapTriplets({
        (pid: PartitionID, iter: Iterator[EdgeTriplet[Histogram, Topic]]) =>
          val gen = new java.util.Random(parts * interIter + pid)
          iter.map({ token =>
            val u = gen.nextDouble()
            val newTopic = sampleToken(u, token.attr, token.dstAttr,
              token.srcAttr, totalHistogramBroadcast.value.get,
              nt, token.dstAttr.docTopics, a, b, nw)
            newTopic
          })
      }, TripletFields.All).cache

      if (loggingTime && iteration % loggingInterval == 0 && iteration > 0) {
        graph.cache().triplets.count()
        resampleTimes.get += System.nanoTime() - tempTimer
      }

      // Update the counts
      updateGraph();
      this
    }
    
    def updateGraph(): Unit = {
      var tempTimer = System.nanoTime()
      val deltas = graph.edges
        .flatMap(e => {
          val topic = e.attr
          val old = getOldTopic(topic)
          val current = getCurrentTopic(topic)
          var result: Iterator[(VertexId, Topic)] = Iterator.empty
          if (old != current) {
            result = Iterator((e.srcId, e.attr), (e.dstId, e.attr))
          }
          result
        })
        .aggregateByKey(new Array[Int](nTopics + nDocTopics))(combineDeltaIntoCounts, combineCounts)
        .cache()

      graph = graph.outerJoinVertices(deltas)({ (vid, oldHistogram, vertexDeltasOption) =>
        if (vertexDeltasOption.isDefined) {
          val vertexDeltas = vertexDeltasOption.get
          // val parameter = if (vid < 0) a else b
          val histogram = applyDeltasToHistogram(oldHistogram, vertexDeltas, vid)
          histogram
        } else {
          Histogram(oldHistogram.counts, oldHistogram.docTopics)
        }
      }).cache()

      if (loggingTime && iteration % loggingInterval == 0 && iteration > 0) {
        graph.cache().triplets.count()
        updateCountsTimes.get += System.nanoTime() - tempTimer
      }

      // Recompute the global counts (the actual action)
      tempTimer = System.nanoTime()
      totalHistogram = Option(makeHistogramFromCounts(graph.edges.map(e => e.attr)
        .aggregate(new Array[Int](nTopics + nDocTopics))(combineTopicIntoCounts, combineCounts),
        Array()))
      assert(totalHistogram.get.counts.sum == nTokens)

      if (loggingTime && iteration % loggingInterval == 0 && iteration > 0) {
        globalCountsTimes.get += System.nanoTime() - tempTimer
      }

      iteration += 1

    }

    /**
     * Creates an object holding the top counts. The first array is of size number
     * of topics. It contains a list of k elements representing the top words for that topic
     * @param k Number of top words to output
     * @return object with top counts for each word.
     */
    def topWords(count: Int = 0): Array[Array[(Int, WordId)]] = {
      var k = count
      if (k < 1) k = posterior.words.count().toInt

      val nt = nTopics + nDocTopics
      graph.vertices.filter({
        case (vid, c) => vid >= 0
      }).mapPartitions({ items =>
        // TODO: Make this a bounded queue
        val queues = Array.fill(nt)(new PriorityQueue[(Int, WordId)]())
        for ((wordId, factor) <- items) {
          var t = 0
          while (t < nt) {
            val tpl: (Int, WordId) = (factor.counts(t), wordId)
            queues(t) += tpl
            t += 1
          }
        }
        Iterator(queues)
      }).reduce({ (q1, q2) =>
        q1.zip(q2).foreach({ case (a, b) => a ++= b })
        q1
      }).map(q => q.take(count).toArray)
    }

    def summarizeDocGroups(i: Int = 0): Array[(GroupId, DocId, Double)] = {
      val a = alpha
      val b = beta
      val nt = nTopics
      val nd = nDocTopics
      val nw = nWords
      val totalHist = sc.get.broadcast(totalHistogram)
      val didVsSrc = graph.triplets.map(f => (f.dstId, f.srcAttr.counts)).groupBy(_._1)
      val docTriplets = graph.triplets.groupBy(f => (f.dstId))
      val res = docTriplets.join(didVsSrc)
      val docKLScores = res.map({
        case (docId, (itr, srcItr)) =>
          val docWords = itr.groupBy(f => f.srcId)
          var groupTopicIndex = -1L
          var klScore = 0.0
          docWords.foreach(word => {
            val docWordCount = itr.size
            // All elements in this itr point to the same vertex so just take the head
            val doc = itr.head.dstAttr
            val wordHist = itr.head.srcAttr.counts
            val docHist = doc.counts
            groupTopicIndex = doc.docTopics(i)
            val w = wordHist(groupTopicIndex.toInt)
            val d = docHist(groupTopicIndex.toInt) //
            val total = totalHist.value.get.counts(groupTopicIndex.toInt)
            val phi = (b(nt + i) + w) / (b(nt + i) * nw + total)
            val wordProbInDoc = docWordCount.toDouble / doc.counts.sum.toDouble
            klScore += phi * math.log(phi / wordProbInDoc)
          })
          // TODO: For now this gets the job done but very very expensive
          srcItr.foreach({
            case (did, wordHist) => {
              if (did != docId) {
                val w = wordHist(groupTopicIndex.toInt)
                val total = totalHist.value.get.counts(groupTopicIndex.toInt)
                var wbg = 0
                var totalbg = 0
                for (x <- 0 to nt) {
                  wbg += wordHist(x)
                  totalbg += totalHist.value.get.counts(x)
                }
                val phi = (b(nt + i) + w) / (b(nt + i) * nw + total)
                val wordProbInBg = (b(0) + wbg) / (b(0) * nw + totalbg)
                klScore += phi * math.log(phi / wordProbInBg)
              }
            }
          })
          (groupTopicIndex, docId, klScore)
      })
      docKLScores.collect()
    }
    /**
     * Creates the posterior distribution for sampling from the vertices
     * @return Posterior distribution
     */
    def posterior: Posterior = {
      graph.cache()
      val words = graph.vertices.filter({ case (vid, _) => vid >= 0 })
      val docs = graph.vertices.filter({ case (vid, _) => vid < 0 })
      new Posterior(words, docs)
    }

    def logLikelihood(): Double = {
      /* val nw = nWords
      val nt = nTopics
      val nd = nDocs
      val a = alpha
      val b = beta
      val logAlpha = Gamma.logGamma(a)
      val logBeta = Gamma.logGamma(b)
      val logPWGivenZ =
        nTopics * (Gamma.logGamma(nw * b) - nw * logBeta) -
          totalHistogram.counts.map(v => Gamma.logGamma(v + nw * b)).sum +
          wordVertices.map({ case (id, histogram) => 
          histogram.counts.map(v => Gamma.logGamma(v + b)).sum})
            .reduce(_ + _)
      val logPZ =
        nd * (Gamma.logGamma(nt * a) - nt * logAlpha) +
          docVertices.map({ case (id, histogram) =>
            histogram.counts.map(v => 
            Gamma.logGamma(v + a)).sum - Gamma.logGamma(nt * a + histogram.counts.sum)
          }).reduce(_ + _)
      logPWGivenZ + logPZ */
      throw new NotImplementedException
    }

    /**
     * Logs the final machine performance and ML performance to INFO
     */
    def logPerformanceStatistics(): Unit = {
      val resampleTime = if (loggingTime) resampleTimes.get.reduce(_ + _) / 1e9 else 0
      val updateCountsTime = if (loggingTime) updateCountsTimes.get.reduce(_ + _) / 1e9 else 0
      val globalCountsTime = if (loggingTime) globalCountsTimes.get.reduce(_ + _) / 1e9 else 0
      val likelihoodList = likelihoods.toList
      val likelihoodString = likelihoodList.mkString(",")
      val finalLikelihood = likelihoodList.last
      logInfo("LDA Model Parameters and Information")
      logInfo(s"Topics: $nTopics")
      logInfo(s"Alpha: $alpha")
      logInfo(s"Beta: $beta")
      logInfo(s"Number of Documents: $nDocs")
      logInfo(s"Number of Words: $nWords")
      logInfo(s"Number of Tokens: $nTokens")
      logInfo("Running Time Statistics")
      /* logInfo(s"Setup: $setupTime s")
      logInfo(s"Run: $runTime s")
      logInfo(s"Total: $totalTime s") */
      if (loggingTime) {
        logInfo(s"Resample Time: $resampleTime")
        logInfo(s"Update Counts Time: $updateCountsTime")
        logInfo(s"Global Counts Time: $globalCountsTime")
      }
      logInfo("Machine Learning Performance")
      if (loggingLikelihood) {
        logInfo(s"Likelihoods: $likelihoodString")
      }
      logInfo(s"Final Log Likelihood: $finalLikelihood")
    }

    /* private def setupTime: Double = {
      timer.getSeconds("setup")
    }
    private def runTime: Double = {
      timer.getSeconds("run")
    }
    private def totalTime: Double = {
      timer.getSeconds("setup") + timer.getSeconds("run")
    } */

  }
}
